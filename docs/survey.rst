Literature Survey
===================

Show and Tell: A Neural Image Caption Generator 
--------------------------------------------------
It is a simple end-to-end model :cite:`vinyals2015show` which is built on top of a Convolutional Neural Network (CNN) and a special type of Recurrent Neural Network called as Long Short Term Memory Network (LSTM) . At initial time step visual are used in LSTMs as input. After first time step, vector representations of sentence words becomes input parameters of recurrent network. Word embeddings are also learned during training. Sampling and BeamSearch methods are used for inference. The proposed framework is tested on five different public datasets and it was the state-of-art by then. Generation and ranking results are reported using several evaluation metrics. There's also an `TensorFlow implementation <https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow>`_

.. figure:: static/show-and-tell.png
   :align: center
   :scale: 100%
   :alt: Model Overview

   Model Overview

Long-term Recurrent Convolutional Networks for Visual Recognition and Description
-------------------------------------------------------------------------------------
This paper :cite:`donahue2015long` presents solutions for image captioning (as well as video captioning and activity recognition). Its framework is also a combination of a CNN and LSTMs. Unlike :cite:`vinyals2015show`, visual features are used in each time step. Also, it proposes *factorization* property for LSTM networks and makes experiments with three different models. It uses a different inference strategy (I didn't figured out it clearly yet) together with BeamSearch. The results are competitive. What's more, it has `Caffe implementation <https://github.com/BVLC/caffe/pull/2033/commits/668b17ede1e31a1d4a2663bd81357ab92065f812>`_.

.. figure:: static/lrcn.png
   :align: center
   :scale: 100%
   :alt: Model Overview

   Model Overview

Show, attend and tell: Neural image caption generation with visual attention
-----------------------------------------------------------------------------
This paper :cite:`xu2015show` is the first neural caption generation with visual `attention <www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/>`_. It uses two different attention mechanisms. Its model also very similar to :cite:`vinyals2015show` and extends it with attention. There are two implementations on GitHub with `Theano <https://github.com/kelvinxu/arctic-captions>`_ and `TensorFlow <https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow>`_. The model outperformed previous state-of-art work.

**Disclaimer**: I understand the intention behind attention mechanism. However, I really get lost when I try to understand mathematical foundations. I guess I will get the full idea when I attemp to implement model.

.. figure:: static/show-attend-and-tell.png
   :align: center
   :scale: 100%
   :alt: Model Overview

   Model Overview

Deep Visual-Semantic Alignments for Generating Image Descriptions
----------------------------------------------------------------------
This paper :cite:`karpathy2015deep` presents a novel framework which consists of two parts: visual-language data alignment and multimodal RNN (m-RNN) for generating descriptions. In the alignment objective, image features are generated with Region Convolutional Neural Network (RCNN) and caption sentences are represented with bidirectional RNN (BRNN) built on LSTMs. In the proposed generative m-RNN framework, input can be full image and their sentence descriptions as well as image region and region description pairs generated by the alignment framework. Also, it has a `Torch implementation <https://github.com/karpathy/neuraltalk2/>`_  (without alignment part) on Github. Finally, the authors extended this work in the follow up paper :cite:`johnson2015densecap`.

.. figure:: static/neuraltalk2.png
   :align: center
   :scale: 100%
   :alt: Model Overview

   Model Overview (Only generation, without alignment)


.. bibliography:: survey.bib
